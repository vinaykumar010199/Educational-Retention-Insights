# -*- coding: utf-8 -*-
"""student-dropout-analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SYK4rl_go9PeLwOb28D54aTG6XgsMcgm

## Student Dropout Analysis

**Columns:**

| **Column Name**                     | **Description**                                                                                         |
|---------------------------------|-----------------------------------------------------------------------------------------------------|
| Marital status                  | The marital status of the student. (Categorical)                                                     |
| Application mode                | The method of application used by the student. (Categorical)                                         |
| Application order               | The order in which the student applied. (Numerical)                                                  |
| Course                          | The course taken by the student. (Categorical)                                                       |
| Daytime/evening attendance      | Whether the student attends classes during the day or in the evening. (Categorical)                 |
| Previous qualification          | The qualification obtained by the student before enrolling in higher education. (Categorical)       |
| Nationality                     | The nationality of the student. (Categorical)                                                         |
| Mother's qualification          | The qualification of the student's mother. (Categorical)                                              |
| Father's qualification          | The qualification of the student's father. (Categorical)                                              |
| Mother's occupation             | The occupation of the student's mother. (Categorical)                                                 |
| Father's occupation             | The occupation of the student's father. (Categorical)                                                 |
| Displaced                       | Whether the student is a displaced person. (Categorical)                                             |
| Educational special needs       | Whether the student has any special educational needs. (Categorical)                                 |
| Debtor                          | Whether the student is a debtor. (Categorical)                                                       |
| Tuition fees up to date         | Whether the student's tuition fees are up to date. (Categorical)                                      |
| Gender                          | The gender of the student. (Categorical)                                                               |
| Scholarship holder              | Whether the student is a scholarship holder. (Categorical)                                            |
| Age at enrollment               | The age of the student at the time of enrollment. (Numerical)                                         |
| International                   | Whether the student is an international student. (Categorical)                                        |
| Curricular units 1st sem (credited) | The number of curricular units credited by the student in the first semester. (Numerical)       |
| Curricular units 1st sem (enrolled) | The number of curricular units enrolled by the student in the first semester. (Numerical)       |
| Curricular units 1st sem (evaluations) | The number of curricular units evaluated by the student in the first semester. (Numerical)   |
| Curricular units 1st sem (approved) | The number of curricular units approved by the student in the first semester. (Numerical)     |

### 1.1.) Importing Required Libraries
"""

import pandas as pd
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_score,
    recall_score
)

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn import svm

#from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import VotingClassifier

from google.colab import drive
drive.mount('/content/drive')
# Load the dataset into a Pandas dataframe

fp = "/content/drive/MyDrive/ml project/dataset.csv"
data= pd.read_csv(fp, encoding='latin-1')



data

data.head()

data.info()

"""### 2) Data Cleaning and Preprocessing"""

data.rename(columns = {'Nacionality':'Nationality', 'Age at enrollment':'Age','ï»¿Marital status':'Marital status'}, inplace = True)

"""### A) Checking for null values"""

data.isnull().sum() #calculate the number of missing values in each column of the DataFrame

data.isna().sum()/len(data)*100 #computes the percentage of missing values((NaN or None).) for each column in the DataFrame data.

"""# duplicated rows"""

# Display number of duplicated rows
print("Number of duplicated rows: ",data.duplicated().sum())

# Display rows with duplicates
print(data[data.duplicated()])

# Remove duplicate rows
data = data.drop_duplicates()

# Verify that duplicates are removed
print("Number of duplicated rows after removal:", data.duplicated().sum())

"""# Get Unique Values"""

# Get no of Unique values in every column
for col in data.columns:
    if len(data[col].unique()) < 10:
        print(f"{col} has {len(data[col].unique())} unique values; Repeated values are {sorted(data[col].unique())}")
    else:
        print(f"{col} has {len(data[col].unique())} values")

"""Hereby we can say that there is no null values in the dataset, which is a good news !
so, we need to do two other steps before moving into the EDA part. They are,
* Encoding the target column(Since it is the only non-numeric field in the dataset)
* Feauture Engineering(Considering only the relevant data to feed our model)
"""

print(data["Target"].unique())

"""So there are 3 unique values in target column which we can replace by
* Dropout - 0
* Enrolled - 1
* Graduate - 2

### 3) Changing 'Dropout' to 0, 'Graduate' to 2 and 'Enrolled' to 1 in 'Target' column
"""

data['Target'] = data['Target'].map({
    'Dropout':0,
    'Enrolled':1,
    'Graduate':2
})

print(data["Target"].unique())

"""Since the number of unique values is less, we used this map() method, if it is large consider using LableEncoder()

So the first part is over.lets move on to the next part.
for the next part we have to,
1. Find how the features are correlated with the Target
2. Remove other unwanted or irrelevant features from the data

## 4.1). Find how the features are correlated with the Target
"""

data.corr()['Target']

"""##4.1.1 Heatmap"""

plt.figure(figsize=(30, 30))
sns.heatmap(data.corr() , annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

new_data = data.copy()
new_data = new_data.drop(columns=['Nationality',
                                  'Mother\'s qualification',
                                  'Father\'s qualification',
                                  'Educational special needs',
                                  'International',
                                  'Curricular units 1st sem (without evaluations)',
                                  'Unemployment rate',
                                  'Inflation rate'], axis=1)
new_data.info()

"""Lets move on to the EDA part

## 5. Exploratory Data Analysis

lets see how many dropouts, enrolled & graduates are there in Target column
"""

new_data['Target'].value_counts()

x = new_data['Target'].value_counts().index
y = new_data['Target'].value_counts().values

df = pd.DataFrame({
    'Target': x,
    'Count_T' : y
})

fig = px.pie(df,
             names ='Target',
             values ='Count_T',
            title='How many dropouts, enrolled & graduates are there in Target column')

fig.update_traces(labels=['Graduate','Dropout','Enrolled'], hole=0.4,textinfo='value+label', pull=[0,0.2,0.1])
fig.show()

"""Let's plot the Top 10 Features with Highest Correlation to Target"""

correlations = data.corr()['Target']
top_10_features = correlations.abs().nlargest(10).index
top_10_corr_values = correlations[top_10_features]

top_10_corr_values

top_10_features

plt.figure(figsize=(10, 11))
plt.bar(top_10_features, top_10_corr_values)
plt.xlabel('Features')
plt.ylabel('Correlation with Target')
plt.title('Top 10 Features with Highest Correlation to Target')
plt.xticks(rotation=45)
plt.show()

"""# Distribution of age of students at the time of enrollment"""

px.histogram(new_data['Age'], x='Age',color_discrete_sequence=['lightgreen'])

"""### Model Training

# 1) Defining Features and Label
"""

X = new_data.drop('Target', axis=1)# features
X.head()

y = new_data['Target'] # label
y.head()

"""2) Splitting the data into training data and testing data"""

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)
#X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)

"""### 3) Training the model"""

from sklearn.svm import SVC
dtree = DecisionTreeClassifier(random_state=0)
rfc = RandomForestClassifier(random_state=2)
lr = LogisticRegression(random_state=42)
knn = KNeighborsClassifier(n_neighbors=3)
svm = SVC(kernel='linear', probability=True, random_state=0)
svm_quadratic = SVC(kernel='poly', degree=2, probability=True, random_state=0)

dtree.fit(X_train,y_train)
rfc.fit(X_train,y_train)
lr.fit(X_train,y_train)
knn.fit(X_train,y_train)
svm.fit(X_train, y_train)
svm_quadratic.fit(X_train, y_train)

# Calculate predictions using dtree model
y_pred_dtree = dtree.predict(X_test)

# Calculate accuracy and store it in a variable
accuracy_dtree = round(accuracy_score(y_test, y_pred_dtree) * 100, 2)

# Print accuracy
print("Accuracy for dtree:", accuracy_dtree, "%")

# Calculate predictions using rfc model
y_pred_rfc = rfc.predict(X_test)

# Calculate accuracy and store it in a variable
accuracy_rfc = round(accuracy_score(y_test, y_pred_rfc) * 100, 2)

# Print accuracy
print("Accuracy for rfc:", accuracy_rfc, "%")

# Calculate predictions using lr model
y_pred_lr = lr.predict(X_test)

# Calculate accuracy and store it in a variable
accuracy_lr = round(accuracy_score(y_test, y_pred_lr) * 100, 2)

# Print accuracy
print("Accuracy for lr:", accuracy_lr, "%")

# Calculate predictions using knn model
y_pred_knn = knn.predict(X_test)

# Calculate accuracy and store it in a variable
accuracy_knn = round(accuracy_score(y_test, y_pred_knn) * 100, 2)

# Print accuracy
print("Accuracy for knn:", accuracy_knn, "%")

# Calculate predictions using svm model
y_pred_svm = svm.predict(X_test)

# Calculate accuracy and store it in a variable
accuracy_svm = round(accuracy_score(y_test, y_pred_svm) * 100, 2)

# Print accuracy
print("Accuracy for svm:", accuracy_svm, "%")

predictions_quadratic = svm_quadratic.predict(X_test)
accuracy_quadratic = round(accuracy_score(y_test, predictions_quadratic) * 100, 2)
print("Accuracy of SVM with quadratic kernel:", accuracy_quadratic, "%")

"""So, Lets improve our accuracy using Ensemble Voting Classifier"""

ens1 = VotingClassifier(estimators=[('rfc', rfc), ('lr', lr),('dtree',dtree),('svm',svm),('svm_quadratic',svm_quadratic)], voting='soft')
ens1.fit(X_train, y_train)

# Calculate predictions using ens1 model
y_pred_ens1 = ens1.predict(X_test)

# Calculate accuracy and store it in a variable
accuracy_ens1 = round(accuracy_score(y_test, y_pred_ens1) * 100, 2)

# Print accuracy
print("Accuracy for ens1:", accuracy_ens1, "%")

ens2 = VotingClassifier(estimators=[('rfc', rfc), ('lr', lr),('dtree',dtree),('svm',svm),('svm_quadratic',svm_quadratic)], voting='hard')
ens2.fit(X_train, y_train)

# Calculate predictions using ens2 model
y_pred_ens2 = ens2.predict(X_test)

# Calculate accuracy and store it in a variable
accuracy_ens2 = round(accuracy_score(y_test, y_pred_ens2) * 100, 2)

# Print accuracy
print("Accuracy for ens2:", accuracy_ens2, "%")

"""### Creating a dataframe out of all the accuracy scores of the these model"""

dataframe_Accuracy_Score_Testing = pd.DataFrame({
    'Model Name': [
    'Decision Tree Classifier',
    'Random Forest Classifier',
    'Logistic Regression',
    'K-Nearest Neighbors Classifier',
    'Support Vector Classifier Linear',
    'Support Vector Classifier Quadratic'
],

    'testing Accuracy': [
    accuracy_dtree,
    accuracy_rfc,
    accuracy_lr,
    accuracy_knn,
    accuracy_svm,
    accuracy_quadratic
],
})

dataframe_for_ensvoting_classifier = pd.DataFrame({
    ' ensemble voting': [
    'Soft',
    'Hard'

],

    'testing Accuracy': [
    accuracy_ens1,
    accuracy_ens2
],
})

dataframe_Accuracy_Score_Testing

dataframe_for_ensvoting_classifier

"""## Conclusion

Random Forest and XGBoost demonstrated the most promising performance among the models tested, while Logistic Regression and the Support Vector Classifier with Quadratic Kernel also showed competitive accuracies.

## Generating Pickle File out of the RandomForestClassifier Model
"""

import pickle

# Create and train your RandomForestClassifier model
rfc = RandomForestClassifier(random_state=2)
rfc.fit(X_train, y_train)


file_name = 'droupout.pkl'

# Save the model to a file using pickle
with open(file_name, 'wb') as file:
    pickle.dump(rfc, file)